# File generated by CompCert 2.4
# Command line: -stdlib /home/michael/.opam/4.02.1/lib/compcert/ -fstruct-return -dasm -lcompcert -I /home/michael/.opam/4.02.1/share/compcert-bench/raytracer /home/michael/.opam/4.02.1/share/compcert-bench/raytracer/memory.c /home/michael/.opam/4.02.1/share/compcert-bench/raytracer/gmllexer.c /home/michael/.opam/4.02.1/share/compcert-bench/raytracer/gmlparser.c /home/michael/.opam/4.02.1/share/compcert-bench/raytracer/eval.c /home/michael/.opam/4.02.1/share/compcert-bench/raytracer/arrays.c /home/michael/.opam/4.02.1/share/compcert-bench/raytracer/vector.c /home/michael/.opam/4.02.1/share/compcert-bench/raytracer/matrix.c /home/michael/.opam/4.02.1/share/compcert-bench/raytracer/object.c /home/michael/.opam/4.02.1/share/compcert-bench/raytracer/intersect.c /home/michael/.opam/4.02.1/share/compcert-bench/raytracer/surface.c /home/michael/.opam/4.02.1/share/compcert-bench/raytracer/light.c /home/michael/.opam/4.02.1/share/compcert-bench/raytracer/simplify.c /home/michael/.opam/4.02.1/share/compcert-bench/raytracer/render.c /home/michael/.opam/4.02.1/share/compcert-bench/raytracer/main.c -lm
	.text
	.align	16
	.globl dotproduct
dotproduct:
	.cfi_startproc
	subl	$20, %esp
	.cfi_adjust_cfa_offset	20
	leal	24(%esp), %edx
	movl	%edx, 0(%esp)
	movl	0(%edx), %eax
	movl	4(%edx), %ecx
	movsd	0(%eax), %xmm0
	movsd	0(%ecx), %xmm6
	mulsd	%xmm6, %xmm0
	movsd	8(%eax), %xmm2
	movsd	8(%ecx), %xmm5
	mulsd	%xmm5, %xmm2
	addsd	%xmm2, %xmm0
	movsd	16(%eax), %xmm1
	movsd	16(%ecx), %xmm3
	mulsd	%xmm3, %xmm1
	addsd	%xmm1, %xmm0
	movsd	%xmm0, 8(%esp)
	fldl	8(%esp)
	addl	$20, %esp
	ret
	.cfi_endproc
	.type	dotproduct, @function
	.size	dotproduct, . - dotproduct
	.text
	.align	16
	.globl between
between:
	.cfi_startproc
	subl	$20, %esp
	.cfi_adjust_cfa_offset	20
	leal	24(%esp), %edx
	movl	%edx, 0(%esp)
	movl	%ebx, 4(%esp)
	movl	%esi, 8(%esp)
	movl	0(%edx), %esi
	movl	4(%edx), %ebx
	movl	8(%edx), %ecx
	movsd	0(%ebx), %xmm1
	movsd	0(%esi), %xmm5
	subsd	%xmm5, %xmm1
	movsd	%xmm1, 0(%ecx)
	movsd	8(%ebx), %xmm2
	movsd	8(%esi), %xmm3
	subsd	%xmm3, %xmm2
	movsd	%xmm2, 8(%ecx)
	movsd	16(%ebx), %xmm0
	movsd	16(%esi), %xmm4
	subsd	%xmm4, %xmm0
	movsd	%xmm0, 16(%ecx)
	movl	4(%esp), %ebx
	movl	8(%esp), %esi
	addl	$20, %esp
	ret
	.cfi_endproc
	.type	between, @function
	.size	between, . - between
	.text
	.align	16
	.globl opposite
opposite:
	.cfi_startproc
	subl	$12, %esp
	.cfi_adjust_cfa_offset	12
	leal	16(%esp), %edx
	movl	%edx, 0(%esp)
	movl	%ebx, 4(%esp)
	movl	0(%edx), %eax
	movl	4(%edx), %ebx
	movsd	0(%eax), %xmm1
	xorpd	__negd_mask, %xmm1
	movsd	%xmm1, 0(%ebx)
	movsd	8(%eax), %xmm0
	xorpd	__negd_mask, %xmm0
	movsd	%xmm0, 8(%ebx)
	movsd	16(%eax), %xmm2
	xorpd	__negd_mask, %xmm2
	movsd	%xmm2, 16(%ebx)
	movl	4(%esp), %ebx
	addl	$12, %esp
	ret
	.cfi_endproc
	.type	opposite, @function
	.size	opposite, . - opposite
	.text
	.align	16
	.globl point_along
point_along:
	.cfi_startproc
	subl	$20, %esp
	.cfi_adjust_cfa_offset	20
	leal	24(%esp), %edx
	movl	%edx, 0(%esp)
	movl	%ebx, 4(%esp)
	movl	%esi, 8(%esp)
	movl	0(%edx), %esi
	movl	4(%edx), %eax
	movsd	8(%edx), %xmm3
	movl	16(%edx), %ebx
	movsd	0(%esi), %xmm5
	movsd	0(%eax), %xmm2
	mulsd	%xmm3, %xmm2
	addsd	%xmm2, %xmm5
	movsd	%xmm5, 0(%ebx)
	movsd	8(%esi), %xmm0
	movsd	8(%eax), %xmm4
	mulsd	%xmm3, %xmm4
	addsd	%xmm4, %xmm0
	movsd	%xmm0, 8(%ebx)
	movsd	16(%esi), %xmm6
	movsd	16(%eax), %xmm1
	mulsd	%xmm3, %xmm1
	addsd	%xmm1, %xmm6
	movsd	%xmm6, 16(%ebx)
	movl	4(%esp), %ebx
	movl	8(%esp), %esi
	addl	$20, %esp
	ret
	.cfi_endproc
	.type	point_along, @function
	.size	point_along, . - point_along
	.text
	.align	16
	.globl product
product:
	.cfi_startproc
	subl	$20, %esp
	.cfi_adjust_cfa_offset	20
	leal	24(%esp), %edx
	movl	%edx, 0(%esp)
	movl	%ebx, 4(%esp)
	movl	%esi, 8(%esp)
	movl	0(%edx), %eax
	movl	4(%edx), %esi
	movl	8(%edx), %ebx
	movsd	8(%eax), %xmm4
	movsd	16(%esi), %xmm3
	mulsd	%xmm3, %xmm4
	movsd	16(%eax), %xmm1
	movsd	8(%esi), %xmm6
	mulsd	%xmm6, %xmm1
	subsd	%xmm1, %xmm4
	movsd	%xmm4, 0(%ebx)
	movsd	16(%eax), %xmm5
	movsd	0(%esi), %xmm0
	mulsd	%xmm0, %xmm5
	movsd	0(%eax), %xmm2
	movsd	16(%esi), %xmm4
	mulsd	%xmm4, %xmm2
	subsd	%xmm2, %xmm5
	movsd	%xmm5, 8(%ebx)
	movsd	0(%eax), %xmm3
	movsd	8(%esi), %xmm2
	mulsd	%xmm2, %xmm3
	movsd	8(%eax), %xmm0
	movsd	0(%esi), %xmm7
	mulsd	%xmm7, %xmm0
	subsd	%xmm0, %xmm3
	movsd	%xmm3, 16(%ebx)
	movl	4(%esp), %ebx
	movl	8(%esp), %esi
	addl	$20, %esp
	ret
	.cfi_endproc
	.type	product, @function
	.size	product, . - product
	.text
	.align	16
	.globl vlength2
vlength2:
	.cfi_startproc
	subl	$20, %esp
	.cfi_adjust_cfa_offset	20
	leal	24(%esp), %edx
	movl	%edx, 0(%esp)
	movl	0(%edx), %eax
	movsd	0(%eax), %xmm5
	movapd	%xmm5, %xmm4
	mulsd	%xmm5, %xmm4
	movsd	8(%eax), %xmm0
	movapd	%xmm0, %xmm1
	mulsd	%xmm1, %xmm0
	addsd	%xmm0, %xmm4
	movsd	16(%eax), %xmm2
	movapd	%xmm2, %xmm3
	mulsd	%xmm3, %xmm2
	addsd	%xmm2, %xmm4
	movsd	%xmm4, 8(%esp)
	fldl	8(%esp)
	addl	$20, %esp
	ret
	.cfi_endproc
	.type	vlength2, @function
	.size	vlength2, . - vlength2
	.text
	.align	16
	.globl vlength
vlength:
	.cfi_startproc
	subl	$28, %esp
	.cfi_adjust_cfa_offset	28
	leal	32(%esp), %edx
	movl	%edx, 8(%esp)
	movl	0(%edx), %eax
	movl	%eax, 0(%esp)
	call	vlength2
	fstpl	16(%esp)
	movsd	16(%esp), %xmm1
	movsd	%xmm1, 0(%esp)
	call	sqrt
	fstpl	16(%esp)
	movsd	16(%esp), %xmm0
	movsd	%xmm0, 16(%esp)
	fldl	16(%esp)
	addl	$28, %esp
	ret
	.cfi_endproc
	.type	vlength, @function
	.size	vlength, . - vlength
	.text
	.align	16
	.globl vscale
vscale:
	.cfi_startproc
	subl	$12, %esp
	.cfi_adjust_cfa_offset	12
	leal	16(%esp), %edx
	movl	%edx, 0(%esp)
	movl	0(%edx), %ecx
	movsd	4(%edx), %xmm4
	movl	12(%edx), %eax
	movsd	0(%ecx), %xmm1
	mulsd	%xmm4, %xmm1
	movsd	%xmm1, 0(%eax)
	movsd	8(%ecx), %xmm0
	mulsd	%xmm4, %xmm0
	movsd	%xmm0, 8(%eax)
	movsd	16(%ecx), %xmm2
	mulsd	%xmm4, %xmm2
	movsd	%xmm2, 16(%eax)
	addl	$12, %esp
	ret
	.cfi_endproc
	.type	vscale, @function
	.size	vscale, . - vscale
	.text
	.align	16
	.globl vnormalize
vnormalize:
	.cfi_startproc
	subl	$44, %esp
	.cfi_adjust_cfa_offset	44
	leal	48(%esp), %edx
	movl	%edx, 16(%esp)
	movl	%ebx, 20(%esp)
	movl	%esi, 24(%esp)
	movl	0(%edx), %ebx
	movl	4(%edx), %esi
	movl	%ebx, 0(%esp)
	call	vlength
	fstpl	32(%esp)
	movsd	32(%esp), %xmm3
	movsd	.L292, %xmm0 # 1
	divsd	%xmm3, %xmm0
	movl	%ebx, 0(%esp)
	movsd	%xmm0, 4(%esp)
	movl	%esi, 12(%esp)
	call	vscale
	movl	20(%esp), %ebx
	movl	24(%esp), %esi
	addl	$44, %esp
	ret
	.cfi_endproc
	.type	vnormalize, @function
	.size	vnormalize, . - vnormalize
	.section	.rodata.cst8,"aM",@progbits,8
	.align	8
.L292:	.quad	0x3ff0000000000000
	.text
	.align	16
	.globl vsub
vsub:
	.cfi_startproc
	subl	$20, %esp
	.cfi_adjust_cfa_offset	20
	leal	24(%esp), %edx
	movl	%edx, 0(%esp)
	movl	%ebx, 4(%esp)
	movl	%esi, 8(%esp)
	movl	0(%edx), %ebx
	movl	4(%edx), %esi
	movl	8(%edx), %ecx
	movsd	0(%ebx), %xmm1
	movsd	0(%esi), %xmm5
	subsd	%xmm5, %xmm1
	movsd	%xmm1, 0(%ecx)
	movsd	8(%ebx), %xmm2
	movsd	8(%esi), %xmm3
	subsd	%xmm3, %xmm2
	movsd	%xmm2, 8(%ecx)
	movsd	16(%ebx), %xmm0
	movsd	16(%esi), %xmm4
	subsd	%xmm4, %xmm0
	movsd	%xmm0, 16(%ecx)
	movl	4(%esp), %ebx
	movl	8(%esp), %esi
	addl	$20, %esp
	ret
	.cfi_endproc
	.type	vsub, @function
	.size	vsub, . - vsub
	.section	.rodata
	.align	16
__negd_mask:	.quad   0x8000000000000000, 0
__absd_mask:	.quad   0x7FFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF
__negs_mask:	.long   0x80000000, 0, 0, 0
__abss_mask:	.long   0x7FFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF
